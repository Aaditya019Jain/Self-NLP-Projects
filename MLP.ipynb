{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11538808,"sourceType":"datasetVersion","datasetId":7236371}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T11:16:19.913913Z","iopub.execute_input":"2025-05-01T11:16:19.914639Z","iopub.status.idle":"2025-05-01T11:16:24.716401Z","shell.execute_reply.started":"2025-05-01T11:16:19.914611Z","shell.execute_reply":"2025-05-01T11:16:24.715288Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\nclass Neural_Network():\n    def __init__(self, neurons, Activations, initialization='randn'):\n        \"\"\"Define the NN design parameters\n        Args:\n            neurons (int array): list of number of neurons in each layer\n            Activations (str array): list of activations to be used for hidden and output layers\n            initialization (str, optional): Choose weight initialization from\n             uniform or normal distribution. Defaults to 'randn'.\n        \"\"\"\n\n        self.inputSize = neurons[0] # Number of neurons in input layer\n        self.outputSize = neurons[-1] # Number of neurons in output layer\n        self.layers = len(neurons)\n        self.weights = [] # weights for each layer\n        self.biases = [] # biases in each layer\n        self.layer_activations = [] # activations for each layer\n\n        if initialization == 'rand':\n            self.initializer = np.random.rand\n        elif initialization == 'randn':\n            self.initializer = np.random.randn\n        else:\n           raise ValueError(\"initialization must be 'rand' or 'randn' or 'he' or 'xavier'\")\n        for i in range(len(neurons)-1):\n            self.weights.append(self.initializer(neurons[i+1],neurons[i])*0.1) # weight matrix between layer i and layer i+1\n            self.biases.append(self.initializer(neurons[i+1],1)*0.1)\n            self.layer_activations.append(Activations[i]) # activations for each layer\n\n\n    def sigmoid(self, z):\n        \"\"\"Returns result of the sigmoid activation function on input z.\"\"\"\n        return 1.0 / (1.0 + np.exp(-z))\n    \n    def sigmoidPrime(self, z):\n        \"\"\"Returns derivative of sigmoid activation function applied on input z.\"\"\"\n        return self.sigmoid(z) * (1 - self.sigmoid(z))\n    \n    def tanh(self, z):\n        \"\"\"Transforms values to -1 to 1\"\"\"\n        return np.tanh(z)\n    \n    def tanhPrime(self, z):\n        \"\"\"Returns derivative of hyperbolic tan activation function applied on input z.\"\"\"\n        return 1 - np.tanh(z)**2\n    \n    def linear(self, z):\n        \"\"\"Returns result of the linear activation function on input z.\"\"\"\n        return z\n    \n    def linearPrime(self, z):\n        \"\"\"Returns derivative of linear activation function applied on input z.\"\"\"\n        return np.ones_like(z)\n    \n    def ReLU(self, z):\n        \"\"\"Returns result of the ReLU activation function on input z.\"\"\"\n        return np.maximum(0, z)\n    \n    def reluPrime(self, z):\n        \"\"\"Returns derivative of ReLU activation function applied on input z.\"\"\"\n        return np.where(z > 0, 1, 0)\n    \n    def softmax(self, z):\n        \"\"\"Returns result of the softmax activation function on input z.\"\"\"\n        e_z = np.exp(z - np.max(z, axis=0))  # Subtract max for numerical stability\n        return e_z / np.sum(e_z, axis=0)\n\n    def lossSE(self, predicted, actual):\n        \"\"\"Implementation of Squared-error loss function.\"\"\"\n        return 0.5 * np.mean(np.sum((predicted - actual)**2, axis=0))\n    \n    def lossCE(self, predicted, actual):\n        \"\"\"Implementation of Cross-Entropy loss function.\"\"\"\n        epsilon = 1e-15  # To avoid log(0)\n        return -np.mean(np.sum(actual * np.log(predicted + epsilon), axis=0))\n\n\n\n    def forward(self, x):\n        \n        layer_activations_a = [x]  # Store the outputs of activation\n        a = x  # Storing input as activation of zero-th layer\n        layer_dot_prod_z = []\n        for i, param in enumerate(zip(self.biases, self.weights)):\n            b, w = param[0], param[1]\n            \n            # Compute z as the weighted sum of inputs plus bias\n            z = np.dot(w, a) + b\n            \n            if self.layer_activations[i].lower() == 'sigmoid':\n                a = self.sigmoid(z)\n            elif self.layer_activations[i].lower() == 'relu':\n                a = self.ReLU(z)\n            elif self.layer_activations[i].lower() == 'tanh':\n                a = self.tanh(z)\n            elif self.layer_activations[i].lower() == 'linear':\n                a = self.linear(z)\n            elif self.layer_activations[i].lower() == 'softmax':\n                a = self.softmax(z)\n            layer_dot_prod_z.append(z)\n            layer_activations_a.append(a)\n        return a, layer_dot_prod_z, layer_activations_a\n\n\n    def backward(self, x, y, zs, activations):\n        grad_b = [np.zeros(b.shape) for b in self.biases]\n        grad_w = [np.zeros(w.shape) for w in self.weights]\n        \n        # Output layer delta calculation\n        delta = (activations[-1] - y)\n        if self.layer_activations[-1].lower() == 'softmax':\n            delta_z = delta\n        else:\n            activation_prime = getattr(self, self.layer_activations[-1].lower() + 'Prime')\n            delta_z = delta * activation_prime(zs[-1])\n        \n        # Average gradients over batch dimension\n        grad_b[-1] = np.mean(delta_z, axis=1, keepdims=True)  # Changed from direct assignment\n        grad_w[-1] = np.dot(delta_z, activations[-2].T) / x.shape[1]\n        \n        # Hidden layers backpropagation\n        for l in range(2, self.layers):\n            current_z = zs[-l]\n            activation_prime = getattr(self, self.layer_activations[-l].lower() + 'Prime')\n            delta_z = np.dot(self.weights[-l+1].T, delta_z) * activation_prime(current_z)\n            \n            # Average gradients over batch dimension\n            grad_b[-l] = np.mean(delta_z, axis=1, keepdims=True)  # Changed from direct assignment\n            grad_w[-l] = np.dot(delta_z, activations[-l-1].T) / x.shape[1]\n            \n        return (grad_b, grad_w)\n\n\n    def update_parameters(self, grads, lr):\n        \n        grad_b, grad_w = grads[0], grads[1]\n        \n        # Update weights and biases using gradient descent\n        for i in range(len(self.weights)):\n            self.weights[i] -= lr * grad_w[i]\n            self.biases[i] -= lr * grad_b[i]\n\n\n    def error(self, X, Y, errors):\n        \"\"\"Appends loss to error list\"\"\"\n        y = np.squeeze(self.forward(X)[0])\n        errors.append(self.loss(y, Y))\n\n    def copy_params(self):\n        \"\"\"Returns a copy of current NN parameters\"\"\"\n        weights = [w.copy() for w in self.weights]\n        biases = [b.copy() for b in self.biases]\n        return (weights, biases)\n    \n\n    def train(self, X, Y, lr = 1e-3, max_epochs = 1000, patience=5, batch_size = None,\n              n_classes=10, onehotencoded=False, loss_func='SE', Xval=None, Yval=None, verbose=True):\n\n\n        if onehotencoded:\n            # a method for creating one hot encoded labels\n            def onehotencoding(Y, n):\n                \"\"\"Convert labels to one-hot encoded format.\n                \n                Args:\n                    Y (array): labels\n                    n (int): number of classes\n                    \n                Returns:\n                    array: one-hot encoded labels\n                \"\"\"\n                \n                # Create a matrix of zeros with shape (n, len(Y))\n                one_hot = np.zeros((n, Y.shape[0]))\n                \n                # Set the corresponding indices to 1\n                one_hot[Y.astype(int), np.arange(Y.shape[0])] = 1\n                \n                return one_hot\n\n\n            Y = onehotencoding(Y, n_classes)\n            if Yval is not None:\n                Yval = onehotencoding(Yval, n_classes)\n\n        # Below code ensures that Y is 2-dimensional even when one-hot encoding is not\n        # performed, so our same code works for training NN for both tasks.\n        Y = np.expand_dims(Y,0) if len(Y.shape) == 1 else Y\n        Yval = np.expand_dims(Yval,0) if (Yval is not None and len(Yval.shape) == 1) else Yval\n\n        if loss_func == 'SE':\n            self.loss = self.lossSE\n        elif loss_func == 'CE':\n            self.loss = self.lossCE\n\n        train_errors=[]\n        if Xval is not None:\n            val_errors=[]\n        i, j, v = 0, 0, np.inf     ## i -> epoch, j->patience, v ->best loss\n        best_params = self.copy_params()\n\n        if batch_size is not None:\n            if batch_size > len(X[0]):\n                raise ValueError(\"invalid mini-batch size. Must be smaller than dataset length\")\n        else:   ## if batch_size is not given\n            batch_size = len(X[0])\n\n        #LR control function\n\n        if isinstance(lr, (float, int)):# constant Ir arg is passed to train get_ir lambda x: 1r # we return a function object which returns the same ir at every epoc elif callable(lr): # function in arg is passed to train\n           get_lr =lambda X: lr # the function is then used to get in for a particular epoch else: raise ValueError('param Ir can only be a number or a scheduler function\")\n        elif callable(lr):\n          get_lr =lr\n        else:\n          raise ValueError('param lr can only be a number or a scheduler function')\n\n\n\n        while j < patience:\n\n            # Shuffle dataset\n            indices = np.random.permutation(X.shape[1])\n            X_shuffled = X[:, indices]\n            Y_shuffled = Y[:, indices]\n            \n            # Train in mini-batches\n            for start_idx in range(0, X.shape[1], batch_size):\n                end_idx = min(start_idx + batch_size, X.shape[1])\n                X_batch = X_shuffled[:, start_idx:end_idx]\n                Y_batch = Y_shuffled[:, start_idx:end_idx]\n                \n                # Forward pass\n                a, zs, activations = self.forward(X_batch)\n                \n                # Backward pass\n                grads = self.backward(X_batch, Y_batch, zs, activations)\n                \n                # Update parameters\n                self.update_parameters(grads, get_lr(i))\n            \n            i += 1  # increment epoch count\n\n            self.error(X, Y, train_errors)   ### appending the loss to train_errors\n            if Xval is not None:\n                self.error(Xval, Yval, val_errors)    ## appending the val_loss to errors\n\n                if val_errors[-1] < v:\n                    j = 0 # reset patience counter\n                    v = val_errors[-1] # update best loss\n                    best_params = self.copy_params() # save params\n                else:\n                    j += 1 # increment patience counter\n\n            if verbose and i%5 == 0:\n                log = f\"Epoch {i}..............Loss on train = {train_errors[-1]}\"\n                if Xval is not None:\n                    log += f\", Loss on val = {val_errors[-1]}\"\n                print(log)\n\n            if i >= max_epochs:\n                break # stop if epoch threshold crossed\n\n        if Xval is not None:\n            if i >= max_epochs and verbose:\n                print(\"Reached Epoch Cap without convergence....Terminating\")\n            elif verbose:\n                print(\"Early Stopping .............. Returning best weights\")\n\n            self.weights, self.biases = best_params # reset to best params\n\n        if verbose:\n            x = np.arange(1, len(train_errors)+1)\n            plt.plot(x, train_errors, label=\"Loss on Train\")\n            if Xval is not None:\n                plt.plot(x, val_errors, label=\"Loss on Val\")\n            plt.legend()\n            plt.title(f\"{loss_func} - Learning Rate = {lr}\")\n            plt.xlabel(\"Epoch\")\n            plt.ylabel(\"Loss\")\n            plt.show()\n        if Xval is not None:\n            return (train_errors, val_errors)\n        return train_errors\n\n    def predict(self, x):\n        \"\"\"Predicts output for the given input\n        \n        Args:\n            x (array): input data\n            \n        Returns:\n            array: predictions\n        \"\"\"\n        return (self.forward(x)[0])\n        \n        # For binary classification\n        # if self.outputSize == 1:\n        #     return (self.forward(x)[0] > 0.5).astype(int)\n        \n        # # For multi-class classification\n        # else:\n        #     return np.argmax(self.forward(x)[0], axis=0)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T18:51:54.525784Z","iopub.execute_input":"2025-05-01T18:51:54.526089Z","iopub.status.idle":"2025-05-01T18:51:54.561853Z","shell.execute_reply.started":"2025-05-01T18:51:54.526063Z","shell.execute_reply":"2025-05-01T18:51:54.560930Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nfrom pathlib import Path\nfrom transformers import BertTokenizer\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.preprocessing import LabelBinarizer\nfrom sklearn.model_selection import train_test_split\n\n\n\ndata_dir = Path('/kaggle/input/fake-new-detection-dataset-mi')\ndf_trn = pd.read_csv(data_dir/'train.csv')\ndf_val = pd.read_csv(data_dir/'valid.csv')\ndf_tst = pd.read_csv(data_dir/'test.csv')\n\ndf_trn['labels'] = df_trn['labels'].map({'true': 1, 'fake': 0})\ndf_val['labels'] = df_val['labels'].map({'true': 1, 'fake': 0})\ndf_tst['labels'] = df_tst['labels'].map({'true': 1, 'fake': 0})\n\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmax_len = 128\n\ndef tokenize_texts(texts, tokenizer, max_len=128):\n    encoded = tokenizer(\n        texts.tolist(),\n        padding='max_length',\n        truncation=True,\n        max_length=max_len,\n        return_tensors=None  # Return NumPy-ready lists\n    )\n    return np.array(encoded['input_ids'])  # [N, max_len]\n\nX_train = tokenize_texts(df_trn['text'], tokenizer, max_len)\nX_test = tokenize_texts(df_tst['text'], tokenizer, max_len)\n\n# Binary labels: make sure they're in shape [N, 1]\ny_train = df_trn['labels'].values.reshape(-1, 1)\ny_test = df_tst['labels'].values.reshape(-1, 1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T18:52:05.008738Z","iopub.execute_input":"2025-05-01T18:52:05.009064Z","iopub.status.idle":"2025-05-01T18:55:00.208257Z","shell.execute_reply.started":"2025-05-01T18:52:05.009039Z","shell.execute_reply":"2025-05-01T18:55:00.207003Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e28c7de60d64d5e9382391b4473e836"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6987ff7f2d0347d6b14aac5179594b63"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4d42711d01148b3b5a864acac56791b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b802de32a2142a39c80ad232df46c48"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    \n    # Create network\n    nn = Neural_Network(neurons=[128, 64, 64, 1], \n                       Activations=['relu','relu', 'sigmoid'],\n                       initialization='randn')\n    \n    # Train network\n    nn.train(X_train.T, y_train.T, lr=0.01, max_epochs=600, verbose=True)\n    \n    # Test predictions\n    Predictions = nn.predict(X_test.T)\n    final_predictions = np.where(Predictions[0]>0.5, 1, 0)\n    print(final_predictions)\n    print(y_test)\n    acc = 0\n        \n    for i in range(len(final_predictions)):\n        if (y_test[i][0] == final_predictions[i]):\n            acc += 1\n    print(acc/len(final_predictions))\n\n# from nnfs.datasets import spiral_data\n# import nnfs\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import accuracy_score\n\n# # Initialize the dataset (fixes the random seed for reproducibility)\n# nnfs.init()\n\n# # Load spiral data\n# X, y = spiral_data(samples=1000, classes=3)\n\n# # Split into train/test (80/20 split)\n# X_train, X_test, y_train, y_test = train_test_split(\n#     X, y, test_size=0.2, random_state=42, stratify=y\n# )\n\n# # Transpose since your neural net expects shape (features, samples)\n# X_train, X_test = X_train.T, X_test.T\n# y_train, y_test = y_train.T, y_test.T\n\n\n# # Train the network\n# nn = Neural_Network(neurons=[2, 16, 16, 3], \n#                     Activations=['relu', 'relu', 'sigmoid'],\n#                     initialization='randn')\n\n# nn.train(X_train, y_train, lr=0.1, max_epochs=500, verbose=True)\n\n# # Predict on test set\n# predictions = nn.predict(X_test)\n# # print(predictions)\n\n# # Convert softmax/sigmoid output to class predictions\n# predicted_classes = np.argmax(predictions, axis=0)\n# true_classes = y_test.flatten()\n\n# # print(predicted_classes)\n# # print(true_classes)\n\n# # Accuracy\n# accuracy = accuracy_score(true_classes, predicted_classes)\n# print(f\"\\nTest Accuracy: {accuracy * 100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:16:01.095302Z","iopub.execute_input":"2025-05-01T19:16:01.095656Z","iopub.status.idle":"2025-05-01T19:17:29.885861Z","shell.execute_reply.started":"2025-05-01T19:16:01.095631Z","shell.execute_reply":"2025-05-01T19:17:29.884472Z"}},"outputs":[{"name":"stdout","text":"Epoch 5..............Loss on train = 0.31426695409912014\nEpoch 10..............Loss on train = 0.3138431851778294\nEpoch 15..............Loss on train = 0.31018170612718343\nEpoch 20..............Loss on train = 0.29263965157963695\nEpoch 25..............Loss on train = 0.27555957843917817\nEpoch 30..............Loss on train = 0.24166671496962866\nEpoch 35..............Loss on train = 0.22440580638437374\nEpoch 40..............Loss on train = 0.21268624957762805\nEpoch 45..............Loss on train = 0.1891401847452183\nEpoch 50..............Loss on train = 0.18780763311425602\nEpoch 55..............Loss on train = 0.18495792937566574\nEpoch 60..............Loss on train = 0.18447917224611995\nEpoch 65..............Loss on train = 0.1841376772450459\nEpoch 70..............Loss on train = 0.18287423219393745\nEpoch 75..............Loss on train = 0.18212292000791122\nEpoch 80..............Loss on train = 0.1818014015120382\nEpoch 85..............Loss on train = 0.18202557212322137\nEpoch 90..............Loss on train = 0.18137670553038615\nEpoch 95..............Loss on train = 0.18125119808762216\nEpoch 100..............Loss on train = 0.18093556829273963\nEpoch 105..............Loss on train = 0.18093556082458034\nEpoch 110..............Loss on train = 0.1809587962485884\nEpoch 115..............Loss on train = 0.18095879615725136\nEpoch 120..............Loss on train = 0.18095879610488771\nEpoch 125..............Loss on train = 0.1809587960709008\nEpoch 130..............Loss on train = 0.18095879604705373\nEpoch 135..............Loss on train = 0.18095879602939782\nEpoch 140..............Loss on train = 0.1809587960158006\nEpoch 145..............Loss on train = 0.18095879600500842\nEpoch 150..............Loss on train = 0.1809587959962359\nEpoch 155..............Loss on train = 0.18095879598896564\nEpoch 160..............Loss on train = 0.1809587959828431\nEpoch 165..............Loss on train = 0.180958795977617\nEpoch 170..............Loss on train = 0.1809587959731043\nEpoch 175..............Loss on train = 0.18095879596916867\nEpoch 180..............Loss on train = 0.18095879596570622\nEpoch 185..............Loss on train = 0.18095879596263673\nEpoch 190..............Loss on train = 0.1809587959598969\nEpoch 195..............Loss on train = 0.18095879595743647\nEpoch 200..............Loss on train = 0.18095879595521475\nEpoch 205..............Loss on train = 0.18095879595319864\nEpoch 210..............Loss on train = 0.18095879595136083\nEpoch 215..............Loss on train = 0.18095879594967862\nEpoch 220..............Loss on train = 0.18095879594813305\nEpoch 225..............Loss on train = 0.18095879594670802\nEpoch 230..............Loss on train = 0.1809587959453899\nEpoch 235..............Loss on train = 0.18095879594416706\nEpoch 240..............Loss on train = 0.18095879594302944\nEpoch 245..............Loss on train = 0.18095879594196834\nEpoch 250..............Loss on train = 0.18095879594097625\nEpoch 255..............Loss on train = 0.18095879594004652\nEpoch 260..............Loss on train = 0.18095879593917344\nEpoch 265..............Loss on train = 0.18095879593835185\nEpoch 270..............Loss on train = 0.1809587959375773\nEpoch 275..............Loss on train = 0.18095879593684577\nEpoch 280..............Loss on train = 0.18095879593615363\nEpoch 285..............Loss on train = 0.18095879593549782\nEpoch 290..............Loss on train = 0.1809587959348754\nEpoch 295..............Loss on train = 0.18095879593428382\nEpoch 300..............Loss on train = 0.18095879593372083\nEpoch 305..............Loss on train = 0.18095879593318426\nEpoch 310..............Loss on train = 0.18095879593267225\nEpoch 315..............Loss on train = 0.18095879593218311\nEpoch 320..............Loss on train = 0.18095879593171527\nEpoch 325..............Loss on train = 0.18095879593126724\nEpoch 330..............Loss on train = 0.1809587959308378\nEpoch 335..............Loss on train = 0.18095879593042574\nEpoch 340..............Loss on train = 0.18095879593002998\nEpoch 345..............Loss on train = 0.18095879592964945\nEpoch 350..............Loss on train = 0.18095879592928327\nEpoch 355..............Loss on train = 0.1809587959289306\nEpoch 360..............Loss on train = 0.18095879592859065\nEpoch 365..............Loss on train = 0.1809587959282626\nEpoch 370..............Loss on train = 0.18095879592794598\nEpoch 375..............Loss on train = 0.18095879592763997\nEpoch 380..............Loss on train = 0.18095879592734404\nEpoch 385..............Loss on train = 0.18095879592705766\nEpoch 390..............Loss on train = 0.18095879592678035\nEpoch 395..............Loss on train = 0.18095879592651162\nEpoch 400..............Loss on train = 0.18095879592625105\nEpoch 405..............Loss on train = 0.18095879592599817\nEpoch 410..............Loss on train = 0.1809587959257527\nEpoch 415..............Loss on train = 0.18095879592551414\nEpoch 420..............Loss on train = 0.1809587959252823\nEpoch 425..............Loss on train = 0.1809587959250568\nEpoch 430..............Loss on train = 0.1809587959248373\nEpoch 435..............Loss on train = 0.18095879592462358\nEpoch 440..............Loss on train = 0.18095879592441536\nEpoch 445..............Loss on train = 0.18095879592421238\nEpoch 450..............Loss on train = 0.18095879592401443\nEpoch 455..............Loss on train = 0.18095879592382125\nEpoch 460..............Loss on train = 0.18095879592363268\nEpoch 465..............Loss on train = 0.18095879592344852\nEpoch 470..............Loss on train = 0.18095879592326852\nEpoch 475..............Loss on train = 0.18095879592309258\nEpoch 480..............Loss on train = 0.18095879592292052\nEpoch 485..............Loss on train = 0.18095879592275213\nEpoch 490..............Loss on train = 0.1809587959225873\nEpoch 495..............Loss on train = 0.1809587959224259\nEpoch 500..............Loss on train = 0.18095879592226774\nEpoch 505..............Loss on train = 0.18095879592211275\nEpoch 510..............Loss on train = 0.1809587959219608\nEpoch 515..............Loss on train = 0.18095879592181172\nEpoch 520..............Loss on train = 0.18095879592166547\nEpoch 525..............Loss on train = 0.18095879592152186\nEpoch 530..............Loss on train = 0.1809587959213809\nEpoch 535..............Loss on train = 0.1809587959212424\nEpoch 540..............Loss on train = 0.1809587959211063\nEpoch 545..............Loss on train = 0.18095879592097255\nEpoch 550..............Loss on train = 0.180958795920841\nEpoch 555..............Loss on train = 0.18095879592071162\nEpoch 560..............Loss on train = 0.18095879592058428\nEpoch 565..............Loss on train = 0.18095879592045896\nEpoch 570..............Loss on train = 0.18095879592033556\nEpoch 575..............Loss on train = 0.18095879592021405\nEpoch 580..............Loss on train = 0.1809587959200943\nEpoch 585..............Loss on train = 0.18095879591997632\nEpoch 590..............Loss on train = 0.18095879591985997\nEpoch 595..............Loss on train = 0.18095879591974529\nEpoch 600..............Loss on train = 0.18095879591963215\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"[0 0 0 ... 0 0 0]\n[[1]\n [1]\n [1]\n ...\n [0]\n [1]\n [1]]\n0.2013607066125567\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"Predictions = nn.predict(X_test.T)\nfinal_predictions = np.where(Predictions[0]>0.5, 1, 0)\nprint(final_predictions)\nprint(y_test)\nacc = 0\n    \nfor i in range(len(final_predictions)):\n    if (y_test[i][0] == final_predictions[i]):\n        acc += 1\nprint(acc/len(final_predictions))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T19:12:38.424195Z","iopub.execute_input":"2025-05-01T19:12:38.424495Z","iopub.status.idle":"2025-05-01T19:12:38.452417Z","shell.execute_reply.started":"2025-05-01T19:12:38.424474Z","shell.execute_reply":"2025-05-01T19:12:38.451539Z"}},"outputs":[{"name":"stdout","text":"[0 0 0 ... 0 0 0]\n[[1]\n [1]\n [1]\n ...\n [0]\n [1]\n [1]]\n0.2013607066125567\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
