{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNRmQTuCInQeEZZxizaew14",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aaditya019Jain/Self-NLP-Projects/blob/main/NUS_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " !huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SV1a9Co0HeVr",
        "outputId": "51b29f71-acc2-4c56-ba06-2942f4bf2869"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `rnd` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `rnd`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchtune torchao triton"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLXtfprmHuGw",
        "outputId": "3b31199a-e33b-4b0b-d62c-ba93c1cd6ec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtune in /usr/local/lib/python3.11/dist-packages (0.5.0)\n",
            "Requirement already satisfied: torchao in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from torchtune) (3.4.1)\n",
            "Requirement already satisfied: huggingface_hub[hf_transfer] in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.29.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.5.3)\n",
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.3.10)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.2.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from torchtune) (0.9.0)\n",
            "Requirement already satisfied: blobfile>=2 in /usr/local/lib/python3.11/dist-packages (from torchtune) (3.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchtune) (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torchtune) (4.67.1)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.11/dist-packages (from torchtune) (2.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from torchtune) (5.9.5)\n",
            "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.11/dist-packages (from torchtune) (11.1.0)\n",
            "Requirement already satisfied: pycryptodomex>=3.8 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.22.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (2.3.0)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (5.3.1)\n",
            "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.11/dist-packages (from blobfile>=2->torchtune) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->torchtune) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (3.11.14)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets->torchtune) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (4.12.2)\n",
            "Requirement already satisfied: hf-transfer>=0.1.4 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub[hf_transfer]->torchtune) (0.1.9)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf->torchtune) (4.9.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->torchtune) (2024.11.6)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->torchtune) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->torchtune) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->torchtune) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets->torchtune) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->torchtune) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->torchtune) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ir-iSOnGz3j"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from torchtune.modules import RotaryPositionalEmbeddings\n",
        "\n",
        "class SimpleLLM(nn.Module):\n",
        "    def __init__(self, model_name, device = \"cpu\"):\n",
        "        super().__init__()\n",
        "        self.device = torch.device(device)  # Store device\n",
        "        self.load_weights(model_name)\n",
        "        self.to(self.device)\n",
        "\n",
        "    def create_attn_layer(self, layer_idx):\n",
        "\n",
        "        layer = nn.ModuleDict({})\n",
        "\n",
        "        # Layer Normalization before attention\n",
        "        layer[\"input_layernorm\"] = nn.RMSNorm(self.hidden_size, eps=1e-5, elementwise_affine=True)\n",
        "        layer[\"input_layernorm\"].weight = nn.Parameter(self.weight[f'model.layers.{layer_idx}.input_layernorm.weight'])\n",
        "\n",
        "        # Self-Attention\n",
        "        layer[\"self_attn\"] = nn.ModuleDict({\n",
        "            \"q_proj\": nn.Linear(self.hidden_size, self.hidden_size, bias=False),\n",
        "            \"k_proj\": nn.Linear(self.hidden_size, self.hidden_size // self.attention_num, bias=False),\n",
        "            \"v_proj\": nn.Linear(self.hidden_size, self.hidden_size // self.attention_num, bias=False),\n",
        "            \"o_proj\": nn.Linear(self.hidden_size, self.hidden_size, bias=False),\n",
        "        })\n",
        "\n",
        "        layer[\"self_attn\"][\"q_proj\"].weight = nn.Parameter(self.weight[f'model.layers.{layer_idx}.self_attn.q_proj.weight'])\n",
        "        layer[\"self_attn\"][\"k_proj\"].weight = nn.Parameter(self.weight[f'model.layers.{layer_idx}.self_attn.k_proj.weight'])\n",
        "        layer[\"self_attn\"][\"v_proj\"].weight = nn.Parameter(self.weight[f'model.layers.{layer_idx}.self_attn.v_proj.weight'])\n",
        "        layer[\"self_attn\"][\"o_proj\"].weight = nn.Parameter(self.weight[f'model.layers.{layer_idx}.self_attn.o_proj.weight'])\n",
        "\n",
        "        # Layer Normalization before MLP\n",
        "        layer[\"post_attn_layernorm\"] = nn.RMSNorm(self.hidden_size, eps=1e-5, elementwise_affine=True)\n",
        "        layer[\"post_attn_layernorm\"].weight = nn.Parameter(self.weight[f'model.layers.{layer_idx}.post_attention_layernorm.weight'])\n",
        "\n",
        "        # MLP Block\n",
        "        layer[\"mlp\"] = nn.ModuleDict({\n",
        "            \"gate_proj\": nn.Linear(self.hidden_size, self.intermediate_size),\n",
        "            \"up_proj\": nn.Linear(self.hidden_size, self.intermediate_size),\n",
        "            \"down_proj\": nn.Linear(self.intermediate_size, self.hidden_size),\n",
        "            \"SILU\": nn.SiLU()\n",
        "        })\n",
        "\n",
        "        layer[\"mlp\"][\"gate_proj\"].weight = nn.Parameter(self.weight[f'model.layers.{layer_idx}.mlp.gate_proj.weight'])\n",
        "        layer[\"mlp\"][\"up_proj\"].weight = nn.Parameter(self.weight[f'model.layers.{layer_idx}.mlp.up_proj.weight'])\n",
        "        layer[\"mlp\"][\"down_proj\"].weight = nn.Parameter(self.weight[f'model.layers.{layer_idx}.mlp.down_proj.weight'])\n",
        "\n",
        "        return layer\n",
        "\n",
        "    def load_weights(self, model_name):\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        self.weight = {k: v.clone().detach() for k, v in model.state_dict().items()}\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        self.config = model.config\n",
        "\n",
        "        del model  # Deleting the model\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        self.hidden_size = self.config.hidden_size\n",
        "        self.num_heads = self.config.num_attention_heads\n",
        "        self.num_layers = self.config.num_hidden_layers\n",
        "        self.vocab_size = self.config.vocab_size\n",
        "        self.intermediate_size = self.config.intermediate_size\n",
        "        self.attention_num = self.num_heads // self.config.num_key_value_heads #confusion - how to reduce the dimension to 512\n",
        "        self.rotary_emb = RotaryPositionalEmbeddings(self.hidden_size // self.num_heads).to(self.device)\n",
        "\n",
        "        # Define embedding layer\n",
        "        self.embed_tokens = nn.Embedding(self.vocab_size, self.hidden_size).to(self.device)\n",
        "        self.embed_tokens.weight = nn.Parameter(self.weight['model.embed_tokens.weight'])\n",
        "\n",
        "        # Define transformer layers\n",
        "        self.layers = nn.ModuleList([self.create_attn_layer(i).to(self.device) for i in range(self.num_layers)])\n",
        "\n",
        "        # Define final layer normalization\n",
        "        self.final_layernorm = nn.RMSNorm(self.hidden_size, eps=1e-5, elementwise_affine=True).to(self.device)\n",
        "        self.final_layernorm.weight = nn.Parameter(self.weight['model.norm.weight'])\n",
        "\n",
        "        # Define final language model head\n",
        "        self.lm_head = nn.Linear(self.hidden_size, self.vocab_size, bias=False).to(self.device)\n",
        "        self.lm_head.weight = nn.Parameter(self.weight['lm_head.weight'])\n",
        "\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"Computes logits for a given input sequence using PyTorch only.\"\"\"\n",
        "        input_ids = input_ids.to(self.device)\n",
        "        batch_size, seq_len = input_ids.shape\n",
        "        x = self.embed_tokens(input_ids)  # [batch, seq_len, hidden_size]\n",
        "\n",
        "        # Apply transformer layers\n",
        "        for layer in self.layers:\n",
        "            residual = x  # Save residual connection\n",
        "\n",
        "            # Layer Norm before Attention\n",
        "            x = layer[\"input_layernorm\"](x)\n",
        "\n",
        "            # Prepare for multi-head attention\n",
        "            batch_size, seq_len, _ = x.shape\n",
        "            head_dim = self.hidden_size // self.num_heads\n",
        "            num_kv_heads = self.num_heads // self.attention_num\n",
        "\n",
        "            # Project for attention\n",
        "            q = layer[\"self_attn\"][\"q_proj\"](x)  # [batch, seq_len, hidden_size]\n",
        "            k = layer[\"self_attn\"][\"k_proj\"](x)  # [batch, seq_len, hidden_size // attention_num]\n",
        "            v = layer[\"self_attn\"][\"v_proj\"](x)  # [batch, seq_len, hidden_size // attention_num]\n",
        "\n",
        "            q, k, v = q.to(self.device), k.to(self.device), v.to(self.device)\n",
        "\n",
        "            # Reshape for multi-head attention\n",
        "            q = q.view(batch_size, seq_len, self.num_heads, head_dim)\n",
        "            k = k.view(batch_size, seq_len, num_kv_heads, head_dim)\n",
        "            v = v.view(batch_size, seq_len, num_kv_heads, head_dim)\n",
        "\n",
        "            # Rotary embeddings\n",
        "            q = self.rotary_emb(q)\n",
        "            k = self.rotary_emb(k)\n",
        "\n",
        "            # Prepare for attention computation\n",
        "            # Transpose for attention: [batch, num_heads, seq_len, head_dim]\n",
        "            q = q.transpose(1, 2)\n",
        "            k = k.transpose(1, 2)\n",
        "            v = v.transpose(1, 2)\n",
        "\n",
        "            # Implement grouped-query attention\n",
        "            # Repeat k and v for each query group\n",
        "            if self.attention_num > 1:\n",
        "                k = k.repeat_interleave(self.attention_num, dim=1)\n",
        "                v = v.repeat_interleave(self.attention_num, dim=1)\n",
        "\n",
        "            # Attention scores: [batch, num_heads, seq_len, seq_len]\n",
        "            attention_scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(head_dim)\n",
        "\n",
        "            # Causal mask (lower triangular)\n",
        "            causal_mask = torch.triu(\n",
        "                torch.ones(seq_len, seq_len, dtype=torch.bool, device=self.device),\n",
        "                diagonal=1\n",
        "            )\n",
        "            attention_scores.masked_fill_(causal_mask, float('-inf'))\n",
        "\n",
        "            # Attention weights: [batch, num_heads, seq_len, seq_len]\n",
        "            attention_weights = F.softmax(attention_scores, dim=-1)\n",
        "\n",
        "            # Apply attention: [batch, num_heads, seq_len, head_dim]\n",
        "            context = torch.matmul(attention_weights, v)\n",
        "\n",
        "            # Reshape back: [batch, seq_len, hidden_size]\n",
        "            context = context.transpose(1, 2).contiguous().view(batch_size, seq_len, self.hidden_size)\n",
        "\n",
        "            # Output projection\n",
        "            attn_output = layer[\"self_attn\"][\"o_proj\"](context)\n",
        "\n",
        "            # Add residual connection\n",
        "            x = residual + attn_output\n",
        "\n",
        "            # Layer Norm before MLP\n",
        "            residual = x  # Save residual connection\n",
        "            x = layer[\"post_attn_layernorm\"](x)\n",
        "\n",
        "            # MLP Block\n",
        "            gate_out = layer[\"mlp\"][\"SILU\"](layer[\"mlp\"][\"gate_proj\"](x))\n",
        "            up_out = layer[\"mlp\"][\"up_proj\"](x)\n",
        "            x = layer[\"mlp\"][\"down_proj\"](gate_out * up_out)\n",
        "\n",
        "            # Add residual connection\n",
        "            x = residual + x\n",
        "\n",
        "        # Apply final normalization\n",
        "        x = self.final_layernorm(x)\n",
        "\n",
        "        logits = self.lm_head(x)  # Compute logits\n",
        "        return logits\n",
        "\n",
        "\n",
        "    def generate(self, prompt, max_length=512):\n",
        "        \"\"\"Generates text token-by-token using greedy decoding.\"\"\"\n",
        "        self.eval()\n",
        "        input_ids = self.tokenizer(prompt, return_tensors='pt').input_ids.to(self.device)\n",
        "\n",
        "        generated = input_ids.clone()\n",
        "\n",
        "        with torch.no_grad():  # Disable gradient computation\n",
        "            for _ in range(max_length):\n",
        "                logits = self.forward(generated)\n",
        "                next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)\n",
        "\n",
        "                generated = torch.cat([generated, next_token], dim=1)\n",
        "\n",
        "                if next_token.item() == self.tokenizer.eos_token_id:\n",
        "                    break\n",
        "\n",
        "        return self.tokenizer.decode(generated.squeeze(), skip_special_tokens=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    model = SimpleLLM(\"meta-llama/Llama-3.2-1B\", device = device)\n",
        "    model.to(\"cuda\")\n",
        "    output = model.generate(\"Once upon a time in a galaxy far, far away\", max_length=25)\n",
        "    print(output)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ydo4dxTwHbV-",
        "outputId": "4f9d26ae-3ade-4052-d7c2-4e483d68c537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Once upon a time in a galaxy far, far away from the earth, the sun was shining, the sun was the sun was the sun was the sun was the time was the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mean squared error = (x- x_true)^2\n",
        "# variance = (x - x_mean)^2"
      ],
      "metadata": {
        "id": "DIFHYbPOJUCv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}